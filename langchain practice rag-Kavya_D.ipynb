{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-huggingface in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (0.24.5)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (0.2.29)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-huggingface) (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.1.98)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.14.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain-huggingface) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain-huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.44.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (0.24.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.43.3)\n",
      "Requirement already satisfied: torch in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bitsandbytes) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->bitsandbytes) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->bitsandbytes) (72.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (3.10.2)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (0.1.98)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\dkavy\\appdata\\roaming\\python\\python312\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "## Libraries Required\n",
    "!pip install langchain-huggingface\n",
    "## For API Calls\n",
    "!pip install huggingface_hub\n",
    "!pip install transformers\n",
    "!pip install accelerate\n",
    "!pip install  bitsandbytes\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dkavy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\dkavy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14317986 -0.23076141 -0.0139477  ...  0.03993539  0.10085298\n",
      "  -0.19941314]\n",
      " [ 0.03999362 -0.20410599 -0.01313243 ...  0.02634657 -0.20096196\n",
      "  -0.1451546 ]]\n"
     ]
    }
   ],
   "source": [
    "## Opensource embedding models from hugging face\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating path for dbs to store and path to fetch the document to embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: c:\\Users\\dkavy\\OneDrive\\Desktop\\Projects\n",
      "File Path: c:\\Users\\dkavy\\OneDrive\\Desktop\\Projects\\books\\us_bill_of_rights.txt\n",
      "Persistent Directory: c:\\Users\\dkavy\\OneDrive\\Desktop\\Projects\\db\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "## getting path details of document to read and embedding vector to store\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "# Define the directory containing the text file and the persistent directory\n",
    "# Define the directory containing the text file and the persistent directory\n",
    "file_path = os.path.join(current_dir, \"books\", \"us_bill_of_rights.txt\")\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db\")\n",
    "\n",
    "print(\"Current Directory:\", current_dir)\n",
    "print(\"File Path:\", file_path)\n",
    "print(\"Persistent Directory:\", persistent_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If vector store already exists, we dont have to create chunks and embed it again, we can directly go for retrival section. Incase, if its not created, create one by running below cells. Those has to come within this if loop. But for experinments how each cell works, I've ran it out of loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already exists. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "# Check if the Chroma vector store already exists\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Vector store already exists. No need to initialize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these if vectore store is not created. This cell does loading the document and chunking it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1190, which is longer than the specified 1000\n",
      "Created a chunk of size 1059, which is longer than the specified 1000\n",
      "Created a chunk of size 1140, which is longer than the specified 1000\n",
      "Created a chunk of size 1696, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 28\n",
      "Sample chunk:\n",
      "ï»¿The Project Gutenberg eBook of The United States Bill of Rights\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The United States Bill of Rights\n",
      "\n",
      "Author: United States\n",
      "\n",
      "Release date: December 1, 1972 [eBook #2]\n",
      "                Most recently updated: April 1, 2015\n",
      "\n",
      "Language: English\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE UNITED STATES BILL OF RIGHTS ***\n",
      "\n",
      "\n",
      "All of the original Project Gutenberg Etexts from the\n",
      "1970's were produced in ALL CAPS, no lower case.  The\n",
      "computers we used then didn't have lower case at all.\n",
      "\n",
      "***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Ensure the text file exists\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"The file {file_path} does not exist. Please check the path.\"\n",
    "    )\n",
    "\n",
    "# Read the text content from the file\n",
    "loader = TextLoader(file_path,encoding='latin1')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display information about the split documents\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is embeddings already created, skip the creation of embeddings again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Hugging face transformers and embediing the chunks and storing in vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating embeddings ---\n",
      "\n",
      "--- Using Hugging Face Transformers ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dkavy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding demonstrations for Hugging Face completed.\n",
      "\n",
      "--- Finished creating embeddings ---\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "class InvalidDimensionException(Exception):\n",
    "    pass\n",
    "\n",
    "print(\"\\n--- Creating embeddings ---\")\n",
    "\n",
    "# Hugging Face Transformers\n",
    "# Uses models from the Hugging Face library.\n",
    "# Ideal for leveraging a wide variety of models for different tasks.\n",
    "# Note: Running Hugging Face models locally on your machine incurs no direct cost other than using your computational resources.\n",
    "# Note: Find other models at https://huggingface.co/models?other=embeddings\n",
    "print(\"\\n--- Using Hugging Face Transformers ---\")\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Attempt to create embeddings\n",
    "Chroma.from_documents(\n",
    "    docs, huggingface_embeddings, persist_directory=persistent_directory)\n",
    "\n",
    "print(\"Embedding demonstrations for Hugging Face completed.\")\n",
    "\n",
    "# Update to a valid embedding model if needed\n",
    "print(\"\\n--- Finished creating embeddings ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrival section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dkavy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# Define the embedding model\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# Load the existing vector store with the embedding function\n",
    "db = Chroma(persist_directory=persistent_directory,\n",
    "            embedding_function=huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking question and retriving the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's question\n",
    "query = \"What is project Gutenberg™\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "Project Gutenbergâ¢ is synonymous with the free distribution of\n",
      "electronic works in formats readable by the widest variety of\n",
      "computers including obsolete, old, middle-aged and new computers. It\n",
      "exists because of the efforts of hundreds of volunteers and donations\n",
      "from people in all walks of life.\n",
      "\n",
      "Volunteers and financial support to provide volunteers with the\n",
      "assistance they need are critical to reaching Project Gutenbergâ¢âs\n",
      "goals and ensuring that the Project Gutenbergâ¢ collection will\n",
      "remain freely available for generations to come. In 2001, the Project\n",
      "Gutenberg Literary Archive Foundation was created to provide a secure\n",
      "and permanent future for Project Gutenbergâ¢ and future\n",
      "generations. To learn more about the Project Gutenberg Literary\n",
      "Archive Foundation and how your efforts and donations can help, see\n",
      "Sections 3 and 4 and the Foundation information page at www.gutenberg.org.\n",
      "\n",
      "Section 3. Information about the Project Gutenberg Literary Archive Foundation\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n",
      "Document 2:\n",
      "Professor Michael S. Hart was the originator of the Project\n",
      "Gutenbergâ¢ concept of a library of electronic works that could be\n",
      "freely shared with anyone. For forty years, he produced and\n",
      "distributed Project Gutenbergâ¢ eBooks with only a loose network of\n",
      "volunteer support.\n",
      "\n",
      "Project Gutenbergâ¢ eBooks are often created from several printed\n",
      "editions, all of which are confirmed as not protected by copyright in\n",
      "the U.S. unless a copyright notice is included. Thus, we do not\n",
      "necessarily keep eBooks in compliance with any particular paper\n",
      "edition.\n",
      "\n",
      "Most people start at our website which has the main PG search\n",
      "facility: www.gutenberg.org.\n",
      "\n",
      "This website includes information about Project Gutenbergâ¢,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "relevant_docs = retriever.invoke(query)\n",
    "# Display the relevant results with metadata\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        source_path = doc.metadata.get('source', 'Unknown')\n",
    "        ## to get once the base path from metadata\n",
    "        source_filename = os.path.basename(source_path)\n",
    "        print(f\"Source: {source_filename}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant_docs contains **metadata**, which has **source** and **page_content** with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'c:\\\\Users\\\\dkavy\\\\OneDrive\\\\Desktop\\\\Projects\\\\books\\\\us_bill_of_rights.txt'}, page_content='Project Gutenbergâ\\x84¢ is synonymous with the free distribution of\\nelectronic works in formats readable by the widest variety of\\ncomputers including obsolete, old, middle-aged and new computers. It\\nexists because of the efforts of hundreds of volunteers and donations\\nfrom people in all walks of life.\\n\\nVolunteers and financial support to provide volunteers with the\\nassistance they need are critical to reaching Project Gutenbergâ\\x84¢â\\x80\\x99s\\ngoals and ensuring that the Project Gutenbergâ\\x84¢ collection will\\nremain freely available for generations to come. In 2001, the Project\\nGutenberg Literary Archive Foundation was created to provide a secure\\nand permanent future for Project Gutenbergâ\\x84¢ and future\\ngenerations. To learn more about the Project Gutenberg Literary\\nArchive Foundation and how your efforts and donations can help, see\\nSections 3 and 4 and the Foundation information page at www.gutenberg.org.\\n\\nSection 3. Information about the Project Gutenberg Literary Archive Foundation'),\n",
       " Document(metadata={'source': 'c:\\\\Users\\\\dkavy\\\\OneDrive\\\\Desktop\\\\Projects\\\\books\\\\us_bill_of_rights.txt'}, page_content='Professor Michael S. Hart was the originator of the Project\\nGutenbergâ\\x84¢ concept of a library of electronic works that could be\\nfreely shared with anyone. For forty years, he produced and\\ndistributed Project Gutenbergâ\\x84¢ eBooks with only a loose network of\\nvolunteer support.\\n\\nProject Gutenbergâ\\x84¢ eBooks are often created from several printed\\neditions, all of which are confirmed as not protected by copyright in\\nthe U.S. unless a copyright notice is included. Thus, we do not\\nnecessarily keep eBooks in compliance with any particular paper\\nedition.\\n\\nMost people start at our website which has the main PG search\\nfacility: www.gutenberg.org.\\n\\nThis website includes information about Project Gutenbergâ\\x84¢,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring different text splitters in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    "    TextSplitter,\n",
    "    TokenTextSplitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1190, which is longer than the specified 1000\n",
      "Created a chunk of size 1059, which is longer than the specified 1000\n",
      "Created a chunk of size 1140, which is longer than the specified 1000\n",
      "Created a chunk of size 1696, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Character-based Splitting ---\n",
      "Sample chunk:\n",
      "ï»¿The Project Gutenberg eBook of The United States Bill of Rights\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The United States Bill of Rights\n",
      "\n",
      "Author: United States\n",
      "\n",
      "Release date: December 1, 1972 [eBook #2]\n",
      "                Most recently updated: April 1, 2015\n",
      "\n",
      "Language: English\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE UNITED STATES BILL OF RIGHTS ***\n",
      "\n",
      "\n",
      "All of the original Project Gutenberg Etexts from the\n",
      "1970's were produced in ALL CAPS, no lower case.  The\n",
      "computers we used then didn't have lower case at all.\n",
      "\n",
      "***\n",
      "\n",
      "Sample chunk:\n",
      "***\n",
      "\n",
      "These original Project Gutenberg Etexts will be compiled into a file\n",
      "containing them all, in order to improve the content ratios of Etext\n",
      "to header material.\n",
      "\n",
      "***\n",
      "\n",
      "The United States Bill of Rights.\n",
      "\n",
      "The Ten Original Amendments to the Constitution of the United States\n",
      "Passed by Congress September 25, 1789\n",
      "Ratified December 15, 1791\n",
      "\n",
      "I\n",
      "\n",
      "Congress shall make no law respecting an establishment of religion,\n",
      "or prohibiting the free exercise thereof; or abridging the freedom of speech,\n",
      "or of the press, or the right of the people peaceably to assemble,\n",
      "and to petition the Government for a redress of grievances.\n",
      "\n",
      "\n",
      "II\n",
      "\n",
      "A well-regulated militia, being necessary to the security of a free State,\n",
      "the right of the people to keep and bear arms, shall not be infringed.\n",
      "\n",
      "\n",
      "III\n",
      "No soldier shall, in time of peace be quartered in any house,\n",
      "without the consent of the owner, nor in time of war,\n",
      "but in a manner to be prescribed by law.\n",
      "\n",
      "\n",
      "IV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Character-based Splitting\n",
    "# Splits text into chunks based on a specified number of characters.\n",
    "# Useful for consistent chunk sizes regardless of content structure.\n",
    "print(\"\\n--- Using Character-based Splitting ---\")\n",
    "# Read the text content from the file\n",
    "loader = TextLoader(file_path,encoding='latin1')\n",
    "documents = loader.load()\n",
    "\n",
    "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "char_docs = char_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "print(f\"Sample chunk:\\n{char_docs[0].page_content}\\n\")\n",
    "print(f\"Sample chunk:\\n{char_docs[1].page_content}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Sentence-based Splitting ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dkavy\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunk:\n",
      "i » ¿ the project gutenberg ebook of the united states bill of rights this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever. you may copy it, give it away or re - use it under the terms of the project gutenberg license included with this ebook or online at www. gutenberg. org. if you are not located in the united states, you will have to check the laws of the country where you are located before using this ebook. title : the united states bill of rights author : united states release date : december 1, 1972 [ ebook # 2 ] most recently updated : april 1, 2015 language : english * * * start of the project gutenberg ebook the united states bill of rights * * * all of the original project gutenberg etexts from the 1970's were produced in all caps, no lower case. the computers we used then didn't have lower case at all. * * * these original project gutenberg etexts will be compiled into a file containing them all, in order to improve the content ratios of etext to header material. * * * the united states bill of rights. the ten original amendments to the constitution of the united states passed by congress september 25, 1789 ratified december 15, 1791 i congress shall make no law respecting an establishment of religion, or prohibiting the free exercise thereof ; or abridging the freedom of speech, or of the press, or the right of the people peaceably to assemble, and to petition the government for a redress of grievances. ii a well - regulated militia, being necessary to the security of a free state, the right of the people to keep and bear arms, shall not be infringed. iii no soldier shall, in time of peace be quartered in any house, without the\n",
      "\n",
      "Sample chunk:\n",
      "regulated militia, being necessary to the security of a free state, the right of the people to keep and bear arms, shall not be infringed. iii no soldier shall, in time of peace be quartered in any house, without the consent of the owner, nor in time of war, but in a manner to be prescribed by law. iv the right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated, and no warrants shall issue, but upon probable cause, supported by oath or affirmation, and particularly describing the place to be searched, and the persons or things to be seized. v no person shall be held to answer for a capital, or otherwise infamous crime, unless on a presentment or indictment of a grand jury, except in cases arising in the land or naval forces, or in the militia, when in actual service in time of war or public danger ; nor shall any person be subject for the same offense to be twice put in jeopardy of life or limb ; nor shall be compelled in any criminal case to be a witness against himself, nor be deprived of life, liberty, or property, without due process of law ; nor shall private property be taken for public use without just compensation. vi in all criminal prosecutions, the accused shall enjoy the right to a speedy and public trial, by an impartial jury of the state and district wherein the crime shall have been committed, which district shall have been previously ascertained by law, and to be informed of the nature and cause of the accusation ; to be confronted with the witnesses against him ; to have compulsory process for obtaining witnesses in his favor, and to have the assistance of counsel for his defense. vii in suits at common law, where the value in controversy shall exceed twenty dollars, the right of trial by jury shall be preserved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the text content from the file\n",
    "loader = TextLoader(file_path,encoding='latin1')\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Sentence-based Splitting\n",
    "# Splits text into chunks based on sentences, ensuring chunks end at sentence boundaries.\n",
    "# Ideal for maintaining semantic coherence within chunks.\n",
    "print(\"\\n--- Using Sentence-based Splitting ---\")\n",
    "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
    "char_docs = sent_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "print(f\"Sample chunk:\\n{char_docs[0].page_content}\\n\")\n",
    "print(f\"Sample chunk:\\n{char_docs[1].page_content}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\dkavy\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.7.0)\n",
      "\n",
      "--- Using Token-based Splitting ---\n",
      "Sample chunk:\n",
      "ï»¿The Project Gutenberg eBook of The United States Bill of Rights\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The United States Bill of Rights\n",
      "\n",
      "Author: United States\n",
      "\n",
      "Release date: December 1, 1972 [eBook #2]\n",
      "                Most recently updated: April 1, 2015\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE UNITED STATES BILL OF RIGHTS ***\n",
      "\n",
      "\n",
      "All of the original Project Gutenberg Etexts from the\n",
      "1970's were produced in ALL CAPS, no lower case.  The\n",
      "computers we used then didn't have lower case at all.\n",
      "\n",
      "***\n",
      "\n",
      "These original Project Gutenberg Etexts will be compiled into a file\n",
      "containing them all, in order to improve the content ratios of Etext\n",
      "to header material.\n",
      "\n",
      "***\n",
      "\n",
      "\n",
      "\n",
      "The United States Bill of Rights.\n",
      "\n",
      "The Ten Original Amendments to the Constitution of the United States\n",
      "Passed by Congress September 25, 1789\n",
      "Ratified December 15, 1791\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "Congress shall make no law respecting an establishment of religion,\n",
      "or prohibiting the free exercise thereof; or abridging the freedom of speech,\n",
      "or of the press, or the right of the people peaceably to assemble,\n",
      "and to petition the Government for a redress of grievances.\n",
      "\n",
      "\n",
      "II\n",
      "\n",
      "A well-regulated militia, being necessary to the security of a free State,\n",
      "the right of the people to keep and bear arms, shall not be infringed.\n",
      "\n",
      "\n",
      "III\n",
      "No soldier shall, in time of peace be quartered in any house,\n",
      "without the consent of the owner, nor in time of war,\n",
      "but in a manner to be prescribed by law.\n",
      "\n",
      "\n",
      "IV\n",
      "\n",
      "The right of the people to be secure in their persons, houses, papers,\n",
      "and effects, against unreasonable searches and seizures, shall not be\n",
      "\n",
      "Sample chunk:\n",
      ", against unreasonable searches and seizures, shall not be violated,\n",
      "and no Warrants shall issue, but upon probable cause, supported by oath\n",
      "or affirmation, and particularly describing the place to be searched,\n",
      "and the persons or things to be seized.\n",
      "\n",
      "\n",
      "V\n",
      "\n",
      "No person shall be held to answer for a capital, or otherwise infamous crime,\n",
      "unless on a presentment or indictment of a Grand Jury, except in cases arising\n",
      "in the land or naval forces, or in the Militia, when in actual service\n",
      "in time of War or public danger; nor shall any person be subject for\n",
      "the same offense to be twice put in jeopardy of life or limb;\n",
      "nor shall be compelled in any criminal case to be a witness against himself,\n",
      "nor be deprived of life, liberty, or property, without due process of law;\n",
      "nor shall private property be taken for public use without just compensation.\n",
      "\n",
      "\n",
      "VI\n",
      "\n",
      "In all criminal prosecutions, the accused shall enjoy the right to a\n",
      "speedy and public trial, by an impartial jury of the State and district\n",
      "wherein the crime shall have been committed, which district shall have\n",
      "been previously ascertained by law, and to be informed of the nature\n",
      "and cause of the accusation; to be confronted with the witnesses against him;\n",
      "to have compulsory process for obtaining witnesses in his favor,\n",
      "and to have the assistance of counsel for his defense.\n",
      "\n",
      "\n",
      "VII\n",
      "\n",
      "In suits at common law, where the value in controversy shall exceed\n",
      "twenty dollars, the right of trial by jury shall be preserved,\n",
      "and no fact tried by a jury shall be otherwise re-examined in any court\n",
      "of the United States, than according to the rules of the common law.\n",
      "\n",
      "\n",
      "VIII\n",
      "\n",
      "Excessive bail shall not be required nor excessive fines imposed,\n",
      "nor cruel and unusual punishments inflicted.\n",
      "\n",
      "\n",
      "IX\n",
      "\n",
      "The enumeration in the Constitution, of certain rights,\n",
      "shall not be construed to deny or disparage others retained by the people.\n",
      "\n",
      "X\n",
      "\n",
      "The powers not delegated to the United States by the Constitution,\n",
      "nor prohibited by it to the States, are reserved to the States respectively,\n",
      "or to the people.\n",
      "\n",
      "\n",
      "\n",
      "*** END OF THE PROJECT GUTENBERG EBOOK THE UNITED STATES BILL OF RIGHTS ***\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Updated editions will replace the previous oneâthe old editions will\n",
      "be renamed.\n",
      "\n",
      "Creating the works from\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement quiet (from versions: none)\n",
      "ERROR: No matching distribution found for quiet\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken -- quiet\n",
    "# Read the text content from the file\n",
    "loader = TextLoader(file_path,encoding='latin1')\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# 3. Token-based Splitting\n",
    "# Splits text into chunks based on tokens (words or subwords), using tokenizers like GPT-2.\n",
    "# Useful for transformer models with strict token limits.\n",
    "print(\"\\n--- Using Token-based Splitting ---\")\n",
    "token_splitter = TokenTextSplitter(chunk_overlap=10, chunk_size=512)\n",
    "token_docs = token_splitter.split_documents(documents)\n",
    "print(f\"Sample chunk:\\n{token_docs[0].page_content}\\n\")\n",
    "print(f\"Sample chunk:\\n{token_docs[1].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Recursive Character-based Splitting ---\n",
      "Sample chunk:\n",
      "ï»¿The Project Gutenberg eBook of The United States Bill of Rights\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Title: The United States Bill of Rights\n",
      "\n",
      "Author: United States\n",
      "\n",
      "Release date: December 1, 1972 [eBook #2]\n",
      "                Most recently updated: April 1, 2015\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE UNITED STATES BILL OF RIGHTS ***\n",
      "\n",
      "\n",
      "All of the original Project Gutenberg Etexts from the\n",
      "1970's were produced in ALL CAPS, no lower case.  The\n",
      "computers we used then didn't have lower case at all.\n",
      "\n",
      "***\n",
      "\n",
      "Sample chunk:\n",
      "All of the original Project Gutenberg Etexts from the\n",
      "1970's were produced in ALL CAPS, no lower case.  The\n",
      "computers we used then didn't have lower case at all.\n",
      "\n",
      "***\n",
      "\n",
      "These original Project Gutenberg Etexts will be compiled into a file\n",
      "containing them all, in order to improve the content ratios of Etext\n",
      "to header material.\n",
      "\n",
      "***\n",
      "\n",
      "\n",
      "\n",
      "The United States Bill of Rights.\n",
      "\n",
      "The Ten Original Amendments to the Constitution of the United States\n",
      "Passed by Congress September 25, 1789\n",
      "Ratified December 15, 1791\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "Congress shall make no law respecting an establishment of religion,\n",
      "or prohibiting the free exercise thereof; or abridging the freedom of speech,\n",
      "or of the press, or the right of the people peaceably to assemble,\n",
      "and to petition the Government for a redress of grievances.\n",
      "\n",
      "\n",
      "II\n",
      "\n",
      "A well-regulated militia, being necessary to the security of a free State,\n",
      "the right of the people to keep and bear arms, shall not be infringed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the text content from the file\n",
    "loader = TextLoader(file_path,encoding='latin1')\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# 4. Recursive Character-based Splitting\n",
    "# Attempts to split text at natural boundaries (sentences, paragraphs) within character limit.\n",
    "# Balances between maintaining coherence and adhering to character limits.\n",
    "print(\"\\n--- Using Recursive Character-based Splitting ---\")\n",
    "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=170)\n",
    "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
    "print(f\"Sample chunk:\\n{rec_char_docs[0].page_content}\\n\")\n",
    "print(f\"Sample chunk:\\n{rec_char_docs[1].page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Custom Splitting ---\n",
      "Sample chunk:\n",
      "ï»¿The Project Gutenberg eBook of The United States Bill of Rights\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this ebook or online\n",
      "at www.gutenberg.org. If you are not located in the United States,\n",
      "you will have to check the laws of the country where you are located\n",
      "before using this eBook.\n",
      "\n",
      "Sample chunk:\n",
      "Title: The United States Bill of Rights\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the text content from the file\n",
    "loader = TextLoader(file_path,encoding='latin1')\n",
    "documents = loader.load()\n",
    "\n",
    "\n",
    "# 5. Custom Splitting\n",
    "# Allows creating custom splitting logic based on specific requirements.\n",
    "# Useful for documents with unique structure that standard splitters can't handle.\n",
    "print(\"\\n--- Using Custom Splitting ---\")\n",
    "\n",
    "\n",
    "class CustomTextSplitter(TextSplitter):\n",
    "    def split_text(self, text):\n",
    "        # Custom logic for splitting text\n",
    "        return text.split(\"\\n\\n\")  # Example: split by paragraphs\n",
    "\n",
    "\n",
    "custom_splitter = CustomTextSplitter()\n",
    "custom_docs = custom_splitter.split_documents(documents)\n",
    "print(f\"Sample chunk:\\n{custom_docs[0].page_content}\\n\")\n",
    "print(f\"Sample chunk:\\n{custom_docs[1].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase different retrieval methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query a vector store with different search types and parameters\n",
    "def query_vector_store(\n",
    "    store_name, query, embedding_function, search_type, search_kwargs\n",
    "):\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- Querying the Vector Store {store_name} ---\")\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory,\n",
    "            embedding_function=embedding_function,\n",
    "        )\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=search_type,\n",
    "            search_kwargs=search_kwargs,\n",
    "        )\n",
    "        relevant_docs = retriever.invoke(query)\n",
    "        # Display the relevant results with metadata\n",
    "        print(f\"\\n--- Relevant Documents for {store_name} ---\")\n",
    "        for i, doc in enumerate(relevant_docs, 1):\n",
    "            print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "            if doc.metadata:\n",
    "                source_path = doc.metadata.get('source', 'Unknown')\n",
    "                ## to get once the base path from metadata\n",
    "                source_filename = os.path.basename(source_path)\n",
    "                print(f\"Source: {source_filename}\\n\")\n",
    "    else:\n",
    "        print(f\"Vector store {store_name} does not exist.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's question\n",
    "query = \"what is Project Gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Similarity Search ---\n",
      "\n",
      "--- Querying the Vector Store chroma_db ---\n",
      "\n",
      "--- Relevant Documents for chroma_db ---\n",
      "Document 1:\n",
      "Project Gutenbergâ¢ is synonymous with the free distribution of\n",
      "electronic works in formats readable by the widest variety of\n",
      "computers including obsolete, old, middle-aged and new computers. It\n",
      "exists because of the efforts of hundreds of volunteers and donations\n",
      "from people in all walks of life.\n",
      "\n",
      "Volunteers and financial support to provide volunteers with the\n",
      "assistance they need are critical to reaching Project Gutenbergâ¢âs\n",
      "goals and ensuring that the Project Gutenbergâ¢ collection will\n",
      "remain freely available for generations to come. In 2001, the Project\n",
      "Gutenberg Literary Archive Foundation was created to provide a secure\n",
      "and permanent future for Project Gutenbergâ¢ and future\n",
      "generations. To learn more about the Project Gutenberg Literary\n",
      "Archive Foundation and how your efforts and donations can help, see\n",
      "Sections 3 and 4 and the Foundation information page at www.gutenberg.org.\n",
      "\n",
      "Section 3. Information about the Project Gutenberg Literary Archive Foundation\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n",
      "Document 2:\n",
      "Professor Michael S. Hart was the originator of the Project\n",
      "Gutenbergâ¢ concept of a library of electronic works that could be\n",
      "freely shared with anyone. For forty years, he produced and\n",
      "distributed Project Gutenbergâ¢ eBooks with only a loose network of\n",
      "volunteer support.\n",
      "\n",
      "Project Gutenbergâ¢ eBooks are often created from several printed\n",
      "editions, all of which are confirmed as not protected by copyright in\n",
      "the U.S. unless a copyright notice is included. Thus, we do not\n",
      "necessarily keep eBooks in compliance with any particular paper\n",
      "edition.\n",
      "\n",
      "Most people start at our website which has the main PG search\n",
      "facility: www.gutenberg.org.\n",
      "\n",
      "This website includes information about Project Gutenbergâ¢,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n",
      "Document 3:\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "    other parts of the world at no cost and with almost no restrictions\n",
      "    whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "    of the Project Gutenberg License included with this eBook or online\n",
      "    at www.gutenberg.org. If you\n",
      "    are not located in the United States, you will have to check the laws\n",
      "    of the country where you are located before using this eBook.\n",
      "  \n",
      "1.E.2. If an individual Project Gutenbergâ¢ electronic work is\n",
      "derived from texts not protected by U.S. copyright law (does not\n",
      "contain a notice indicating that it is posted with permission of the\n",
      "copyright holder), the work can be copied and distributed to anyone in\n",
      "the United States without paying any fees or charges. If you are\n",
      "redistributing or providing access to a work with the phrase âProject\n",
      "Gutenbergâ associated with or appearing on the work, you must comply\n",
      "either with the requirements of paragraphs 1.E.1 through 1.E.7 or\n",
      "obtain permission for the use of the work and the Project Gutenbergâ¢\n",
      "trademark as set forth in paragraphs 1.E.8 or 1.E.9.\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Similarity Search\n",
    "# This method retrieves documents based on vector similarity.\n",
    "# It finds the most similar documents to the query vector based on cosine similarity.\n",
    "# Use this when you want to retrieve the top k most similar documents.\n",
    "print(\"\\n--- Using Similarity Search ---\")\n",
    "query_vector_store(\"chroma_db\", query,\n",
    "                   huggingface_embeddings, \"similarity\", {\"k\": 3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Max Marginal Relevance (MMR)\n",
    "\n",
    "##### Imagine You’re Picking Fruits\n",
    "**Relevance**: You want to pick the ripest fruits (most relevant documents).\n",
    "**Diversity**: You also want a variety of fruits (diverse documents), not just all apples.\n",
    "\n",
    "MMR helps you balance these two goals. It starts by picking the most relevant document (the ripest fruit). Then, for each subsequent pick, it looks for documents that are both relevant and different from the ones already picked.\n",
    "\n",
    "**How It Works**\n",
    "**Initial Selection**: It finds the document most similar to your query.\n",
    "**Iterative Selection**: For each new document, it checks:\n",
    "\n",
    "How similar it is to your query.\n",
    "How different it is from the documents already selected.\n",
    "\n",
    "**k**: This is the number of documents you want to retrieve after applying MMR. In this case, k=2 means you want the top 2 documents that are both relevant and diverse.\n",
    "\n",
    "**fetch_k**: This is the number of top documents to initially fetch based on relevance before applying the MMR algorithm. Here, fetch_k=20 means the algorithm will first fetch the top 20 most relevant documents.\n",
    "\n",
    "**lambda_mult**: This parameter controls the trade-off between relevance and diversity. A value of 0.5 means an equal balance between relevance and diversity. If lambda_mult is closer to 1, the algorithm prioritizes relevance more; if it’s closer to 0, it prioritizes diversity more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Max Marginal Relevance (MMR) ---\n",
      "\n",
      "--- Querying the Vector Store chroma ---\n",
      "\n",
      "--- Relevant Documents for chroma ---\n",
      "Document 1:\n",
      "Project Gutenbergâ¢ is synonymous with the free distribution of\n",
      "electronic works in formats readable by the widest variety of\n",
      "computers including obsolete, old, middle-aged and new computers. It\n",
      "exists because of the efforts of hundreds of volunteers and donations\n",
      "from people in all walks of life.\n",
      "\n",
      "Volunteers and financial support to provide volunteers with the\n",
      "assistance they need are critical to reaching Project Gutenbergâ¢âs\n",
      "goals and ensuring that the Project Gutenbergâ¢ collection will\n",
      "remain freely available for generations to come. In 2001, the Project\n",
      "Gutenberg Literary Archive Foundation was created to provide a secure\n",
      "and permanent future for Project Gutenbergâ¢ and future\n",
      "generations. To learn more about the Project Gutenberg Literary\n",
      "Archive Foundation and how your efforts and donations can help, see\n",
      "Sections 3 and 4 and the Foundation information page at www.gutenberg.org.\n",
      "\n",
      "Section 3. Information about the Project Gutenberg Literary Archive Foundation\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n",
      "Document 2:\n",
      "1.E.3. If an individual Project Gutenbergâ¢ electronic work is posted\n",
      "with the permission of the copyright holder, your use and distribution\n",
      "must comply with both paragraphs 1.E.1 through 1.E.7 and any\n",
      "additional terms imposed by the copyright holder. Additional terms\n",
      "will be linked to the Project Gutenbergâ¢ License for all works\n",
      "posted with the permission of the copyright holder found at the\n",
      "beginning of this work.\n",
      "\n",
      "1.E.4. Do not unlink or detach or remove the full Project Gutenbergâ¢\n",
      "License terms from this work, or any files containing a part of this\n",
      "work or any other work associated with Project Gutenbergâ¢.\n",
      "\n",
      "1.E.5. Do not copy, display, perform, distribute or redistribute this\n",
      "electronic work, or any part of this electronic work, without\n",
      "prominently displaying the sentence set forth in paragraph 1.E.1 with\n",
      "active links or immediate access to the full terms of the Project\n",
      "Gutenbergâ¢ License.\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Max Marginal Relevance (MMR)\n",
    "# This method balances between selecting documents that are relevant to the query and diverse among themselves.\n",
    "# 'fetch_k' specifies the number of documents to initially fetch based on similarity.\n",
    "# 'lambda_mult' controls the diversity of the results: 1 for minimum diversity, 0 for maximum.\n",
    "# Use this when you want to avoid redundancy and retrieve diverse yet relevant documents.\n",
    "# Note: Relevance measures how closely documents match the query.\n",
    "# Note: Diversity ensures that the retrieved documents are not too similar to each other,\n",
    "#       providing a broader range of information.\n",
    "print(\"\\n--- Using Max Marginal Relevance (MMR) ---\")\n",
    "query_vector_store(\n",
    "    \"chroma\",\n",
    "    query,\n",
    "    huggingface_embeddings,\n",
    "    \"mmr\",\n",
    "    {\"k\": 2, \"fetch_k\": 20, \"lambda_mult\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using Similarity Score Threshold ---\n",
      "\n",
      "--- Querying the Vector Store chroma_db ---\n",
      "\n",
      "--- Relevant Documents for chroma_db ---\n",
      "Document 1:\n",
      "Project Gutenbergâ¢ is synonymous with the free distribution of\n",
      "electronic works in formats readable by the widest variety of\n",
      "computers including obsolete, old, middle-aged and new computers. It\n",
      "exists because of the efforts of hundreds of volunteers and donations\n",
      "from people in all walks of life.\n",
      "\n",
      "Volunteers and financial support to provide volunteers with the\n",
      "assistance they need are critical to reaching Project Gutenbergâ¢âs\n",
      "goals and ensuring that the Project Gutenbergâ¢ collection will\n",
      "remain freely available for generations to come. In 2001, the Project\n",
      "Gutenberg Literary Archive Foundation was created to provide a secure\n",
      "and permanent future for Project Gutenbergâ¢ and future\n",
      "generations. To learn more about the Project Gutenberg Literary\n",
      "Archive Foundation and how your efforts and donations can help, see\n",
      "Sections 3 and 4 and the Foundation information page at www.gutenberg.org.\n",
      "\n",
      "Section 3. Information about the Project Gutenberg Literary Archive Foundation\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n",
      "Document 2:\n",
      "Professor Michael S. Hart was the originator of the Project\n",
      "Gutenbergâ¢ concept of a library of electronic works that could be\n",
      "freely shared with anyone. For forty years, he produced and\n",
      "distributed Project Gutenbergâ¢ eBooks with only a loose network of\n",
      "volunteer support.\n",
      "\n",
      "Project Gutenbergâ¢ eBooks are often created from several printed\n",
      "editions, all of which are confirmed as not protected by copyright in\n",
      "the U.S. unless a copyright notice is included. Thus, we do not\n",
      "necessarily keep eBooks in compliance with any particular paper\n",
      "edition.\n",
      "\n",
      "Most people start at our website which has the main PG search\n",
      "facility: www.gutenberg.org.\n",
      "\n",
      "This website includes information about Project Gutenbergâ¢,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n",
      "Document 3:\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "    other parts of the world at no cost and with almost no restrictions\n",
      "    whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "    of the Project Gutenberg License included with this eBook or online\n",
      "    at www.gutenberg.org. If you\n",
      "    are not located in the United States, you will have to check the laws\n",
      "    of the country where you are located before using this eBook.\n",
      "  \n",
      "1.E.2. If an individual Project Gutenbergâ¢ electronic work is\n",
      "derived from texts not protected by U.S. copyright law (does not\n",
      "contain a notice indicating that it is posted with permission of the\n",
      "copyright holder), the work can be copied and distributed to anyone in\n",
      "the United States without paying any fees or charges. If you are\n",
      "redistributing or providing access to a work with the phrase âProject\n",
      "Gutenbergâ associated with or appearing on the work, you must comply\n",
      "either with the requirements of paragraphs 1.E.1 through 1.E.7 or\n",
      "obtain permission for the use of the work and the Project Gutenbergâ¢\n",
      "trademark as set forth in paragraphs 1.E.8 or 1.E.9.\n",
      "\n",
      "Source: us_bill_of_rights.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Similarity Score Threshold\n",
    "# This method retrieves documents that exceed a certain similarity score threshold.\n",
    "# 'score_threshold' sets the minimum similarity score a document must have to be considered relevant.\n",
    "# Use this when you want to ensure that only highly relevant documents are retrieved, filtering out less relevant ones.\n",
    "print(\"\\n--- Using Similarity Score Threshold ---\")\n",
    "query_vector_store(\n",
    "    \"chroma_db\",\n",
    "    query,\n",
    "    huggingface_embeddings,\n",
    "    \"similarity_score_threshold\",\n",
    "    {\"k\": 3, \"score_threshold\": 0.1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding llm to respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\dkavy\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=128,temperature=0.7,token=key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm.invoke(\"What is machine learning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "\n",
    "\n",
    "system=('You are a helpful assistant. Please provide an answer based only on the provided documents. If the answer is not found in the documents, respond with \"Answer not available in data. Please try different query\"')\n",
    "\n",
    "human= (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    ")\n",
    "\n",
    "# Define the messages for the model\n",
    "messages = [\n",
    "    SystemMessage(content=system),\n",
    "    HumanMessage(content=human),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant. Please provide an answer based only on the provided documents. If the answer is not found in the documents, respond with \"Answer not available in data. Please try different query\"'),\n",
       " HumanMessage(content='Here are some documents that might help answer the question: what is Project Gutenberg\\n\\nRelevant Documents:\\nProject Gutenbergâ\\x84¢ is synonymous with the free distribution of\\nelectronic works in formats readable by the widest variety of\\ncomputers including obsolete, old, middle-aged and new computers. It\\nexists because of the efforts of hundreds of volunteers and donations\\nfrom people in all walks of life.\\n\\nVolunteers and financial support to provide volunteers with the\\nassistance they need are critical to reaching Project Gutenbergâ\\x84¢â\\x80\\x99s\\ngoals and ensuring that the Project Gutenbergâ\\x84¢ collection will\\nremain freely available for generations to come. In 2001, the Project\\nGutenberg Literary Archive Foundation was created to provide a secure\\nand permanent future for Project Gutenbergâ\\x84¢ and future\\ngenerations. To learn more about the Project Gutenberg Literary\\nArchive Foundation and how your efforts and donations can help, see\\nSections 3 and 4 and the Foundation information page at www.gutenberg.org.\\n\\nSection 3. Information about the Project Gutenberg Literary Archive Foundation\\n\\nProfessor Michael S. Hart was the originator of the Project\\nGutenbergâ\\x84¢ concept of a library of electronic works that could be\\nfreely shared with anyone. For forty years, he produced and\\ndistributed Project Gutenbergâ\\x84¢ eBooks with only a loose network of\\nvolunteer support.\\n\\nProject Gutenbergâ\\x84¢ eBooks are often created from several printed\\neditions, all of which are confirmed as not protected by copyright in\\nthe U.S. unless a copyright notice is included. Thus, we do not\\nnecessarily keep eBooks in compliance with any particular paper\\nedition.\\n\\nMost people start at our website which has the main PG search\\nfacility: www.gutenberg.org.\\n\\nThis website includes information about Project Gutenbergâ\\x84¢,\\nincluding how to make donations to the Project Gutenberg Literary\\nArchive Foundation, how to help produce our new eBooks, and how to\\nsubscribe to our email newsletter to hear about new eBooks.')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generated Response ---\n",
      "Content only:\n",
      "\n",
      "\n",
      "Answer: Project Gutenberg is a literary archive that provides free electronic works in various formats for various types of computers. It was started by Professor Michael S. Hart and is run by volunteers and donations. The works are sourced from public domain books and can be accessed through the Project Gutenberg website (www.gutenberg.org).\n"
     ]
    }
   ],
   "source": [
    "# Invoke the model with the combined input\n",
    "result = llm.invoke(messages)\n",
    "\n",
    "# Display the full result and content only\n",
    "print(\"\\n--- Generated Response ---\")\n",
    "# print(\"Full result:\")\n",
    "# print(result)\n",
    "print(\"Content only:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 5772, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 5\n",
      "Sample chunk:\n",
      "Apple\n",
      "\n",
      "\n",
      "Apple\n",
      "\n",
      "AppleStoreMaciPadiPhoneWatchVisionAirPodsTV & HomeEntertainmentAccessoriesSupport\n",
      "\n",
      "\n",
      "0+\n",
      "\n",
      " \n",
      "\n",
      "The Relay\n",
      "Apple products are designed for every athlete. And every body.\n",
      "\n",
      "Watch the film\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "iPhone\n",
      "Our most powerful cameras yet. Ultrafast chips. And USB-C.\n",
      "\n",
      "Learn more\n",
      "Shop iPhone\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Buy Mac or iPad for college\n",
      "\n",
      "\n",
      "with education savings\n",
      "\n",
      "Get a gift card up to $150*\n",
      "\n",
      "\n",
      "Only at the Apple Store\n",
      "\n",
      "\n",
      "Shop\n",
      "\n",
      "\n",
      "Buy Mac or iPad for college\n",
      "\n",
      "\n",
      "with education savings\n",
      "\n",
      "Get AirPods with Mac*\n",
      "\n",
      "\n",
      "Apple Pencil with iPad*\n",
      "\n",
      "\n",
      "Only at the Apple Store\n",
      "\n",
      "\n",
      "Shop\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "iPad Pro\n",
      "Unbelievably thin. Incredibly powerful.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "MacBook Air\n",
      "Lean. Mean. M3 machine.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Vision Pro\n",
      "The era of spatial computing is here.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Watch Series 9\n",
      "Smarter. Brighter. Mightier.\n",
      "\n",
      "Learn more\n",
      "Buy\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Apple Card\n",
      "Get up to 3% Daily Cash back with every purchase.\n",
      "\n",
      "Learn more\n",
      "Apply now\n",
      "Apply now\n",
      "\n",
      "Vector store c:\\Users\\dkavy\\OneDrive\\Desktop\\Projects\\db\\chroma_db_apple already exists. No need to initialize.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_apple\")\n",
    "\n",
    "\n",
    "# Step 1: Scrape the content from apple.com using WebBaseLoader\n",
    "# WebBaseLoader loads web pages and extracts their content\n",
    "urls = [\"https://www.apple.com/\"]\n",
    "\n",
    "# Create a loader for web content\n",
    "loader = WebBaseLoader(urls)\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split the scraped content into chunks\n",
    "# CharacterTextSplitter splits the text into smaller chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display information about the split documents\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(docs)}\")\n",
    "print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")\n",
    "\n",
    "# Step 3: Create embeddings for the document chunks\n",
    "# we use huggingface embeddings\n",
    "# Step 4: Create and persist the vector store with the embeddings\n",
    "# Chroma stores the embeddings for efficient searching\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(f\"\\n--- Creating vector store in {persistent_directory} ---\")\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=persistent_directory)\n",
    "    print(f\"--- Finished creating vector store in {persistent_directory} ---\")\n",
    "else:\n",
    "    print(f\"Vector store {persistent_directory} already exists. No need to initialize.\")\n",
    "    db = Chroma(persist_directory=persistent_directory, embedding_function=huggingface_embeddings)\n",
    "\n",
    "# Step 5: Query the vector store\n",
    "# Create a retriever for querying the vector store\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "\n",
      "--- Generated Response ---\n",
      "Content only:\n",
      "Document 1: Apple's Press Release on New MacBook Pro\n",
      "Document 2: Apple's Press Release on New iPhone 13\n",
      "Document 3: Apple's Press Release on New iPad Mini\n",
      "Document 4: Apple's Press Release on New AirPods 3\n",
      "Document 5: Apple's Press Release on New iMac\n",
      "\n",
      "Assistant: Based on the provided documents, Apple has announced the following new products on Apple.com:\n",
      "\n",
      "1. New MacBook Pro\n",
      "2. New iPhone 13\n",
      "3. New iPad Mini\n",
      "4. New AirPods 3\n",
      "5. New iMac.\n",
      "\n",
      "These announcements were made in the respective press releases that you've provided.\n"
     ]
    }
   ],
   "source": [
    "# Define the user's question\n",
    "query = \"What new products are announced on Apple.com?\"\n",
    "\n",
    "# Retrieve relevant documents based on the query\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# Display the relevant results with metadata\n",
    "print(\"\\n--- Relevant Documents ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")\n",
    "        \n",
    "# Retrieve relevant documents based on the query\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2},\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "\n",
    "\n",
    "system=('You are a helpful assistant. Please provide an answer based only on the provided documents. If the answer is not found in the documents, respond with \"Answer not available in data. Please try different query\". Do not use your own knowledge to answer the query')\n",
    "\n",
    "human= (\n",
    "    \"Here are some documents that might help answer the question: \"\n",
    "    + query\n",
    "    + \"\\n\\nRelevant Documents:\\n\"\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    ")\n",
    "\n",
    "# Define the messages for the model\n",
    "messages = [\n",
    "    SystemMessage(content=system),\n",
    "    HumanMessage(content=human),\n",
    "]\n",
    "        \n",
    "# Invoke the model with the combined input\n",
    "result = llm.invoke(messages)\n",
    "\n",
    "# Display the full result and content only\n",
    "print(\"\\n--- Generated Response ---\")\n",
    "# print(\"Full result:\")\n",
    "# print(result)\n",
    "print(\"Content only:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FireCrawl** is a tool that crawls and converts any website into LLM-ready data. It handles complex tasks such as reverse proxies, caching, rate limits, and content blocked by JavaScript3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
